{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Matplotlib inline magic command\n",
    "%matplotlib inline\n",
    "# import dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# import the connect library for psycopg2\n",
    "import psycopg2\n",
    "from psycopg2 import OperationalError, errorcodes, errors\n",
    "\n",
    "from config import db_password\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import boto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://covid-data-finalproject.s3.amazonaws.com/COVID-19_Case_Surveillance_Public_Use_Data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from S3 bucket\n",
    "usa_df = pd.read_csv(\"s3://covid-data-finalproject/COVID-19_Case_Surveillance_Public_Use_Data.csv\",low_memory=False)\n",
    "usa_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all NANs\n",
    "usa_df.dropna(inplace=True)\n",
    "usa_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values from race column\n",
    "usa_df[\"race_ethnicity_combined\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values from sex column\n",
    "usa_df[\"sex\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values from hospitalizations column\n",
    "usa_df[\"hosp_yn\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values from icu column\n",
    "usa_df[\"icu_yn\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values from death column\n",
    "usa_df[\"death_yn\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values from medcond column\n",
    "usa_df[\"medcond_yn\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values from age group column\n",
    "usa_df[\"age_group\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to drop all rows with values=\"Missing\"\n",
    "usa_df = usa_df[(usa_df[\"race_ethnicity_combined\"] != \"Missing\") & (usa_df[\"sex\"] != \"Missing\") & \n",
    "                (usa_df[\"hosp_yn\"] != \"Missing\") & (usa_df[\"age_group\"] != \"Missing\") & (usa_df[\"death_yn\"] != \"Missing\") &\n",
    "               (usa_df[\"medcond_yn\"] != \"Missing\") & (usa_df[\"icu_yn\"] != \"Missing\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify \"Missing\" doesn't show up in Race column\n",
    "usa_df[\"race_ethnicity_combined\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify \"Missing\" doesn't show up in Sex column\n",
    "usa_df[\"sex\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify \"Missing\" doesn't show up in hosp_yn column\n",
    "usa_df[\"hosp_yn\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify \"Missing\" doesn't show up in age_group column\n",
    "usa_df[\"age_group\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify \"Missing\" doesn't show up in medcond column\n",
    "usa_df[\"medcond_yn\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Unknown\" values from \"hosp_yn\" & \"death_yn\" with \"No\"\n",
    "usa_df[\"hosp_yn\"] = usa_df[\"hosp_yn\"].str.replace(\"Unknown\", \"No\")\n",
    "usa_df[\"death_yn\"] = usa_df[\"death_yn\"].str.replace(\"Unknown\", \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a count on cleaned dataset\n",
    "usa_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deaths by Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get death count by grouping on Race and filtering on death_yn = \"Yes\"\n",
    "deaths_by_race = usa_df.groupby(\"race_ethnicity_combined\")[\"death_yn\"].apply(lambda d:(d==\"Yes\").sum()).reset_index(name='death_count')\n",
    "deaths_by_race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-axis & y-axis for plotting bar chart\n",
    "x = deaths_by_race[\"race_ethnicity_combined\"].tolist()\n",
    "y = deaths_by_race[\"death_count\"].tolist()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar chart with x-axis being \"death count\" & y-axis being \"Race/Ethnicity\"\n",
    "fig, ax = plt.subplots(figsize=(8,8)) \n",
    "ax.barh(x,y, color=\"magenta\", label=\"Deaths by Race\")\n",
    "ax.tick_params(axis='x', rotation=80)\n",
    "ax.tick_params(axis='both', labelsize=8)\n",
    "plt.xlabel('Deaths', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Race/Ethnicity', fontsize=12, fontweight='bold')\n",
    "plt.title(\"Deaths_By_Race\", fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "plt.savefig('Resources/Deaths_By_Race.png', transparent=False, facecolor=\"skyblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage of deaths by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of death count\n",
    "all_deaths_by_race = deaths_by_race[\"death_count\"].sum()\n",
    "all_deaths_by_race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of deaths by Race\n",
    "perc_deaths_by_race = ((deaths_by_race[\"death_count\"]/all_deaths_by_race)*100).tolist()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart for \"percentage death\" vs \"Race/Ethnicity\"\n",
    "fig, ax = plt.subplots(figsize=(8,8)) \n",
    "rects = ax.barh(x, perc_deaths_by_race, color=\"green\", label=\"Deaths by Race\")\n",
    "ax.tick_params(axis='x', rotation=80)\n",
    "ax.tick_params(axis='both', labelsize=8)\n",
    "plt.xlabel('Deaths by percentage', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Race/Ethnicity', fontsize=12, fontweight='bold')\n",
    "plt.title(\"Perc_Deaths_By_Race\", fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "plt.savefig('Resources/%_of_Deaths_By_Race.png', transparent=False, facecolor=\"skyblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospitalizations by Age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned dataframe\n",
    "usa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hospitalization count by grouping on Age group and filtering on hosp_yn = \"Yes\"\n",
    "hosp_by_age_group = usa_df.groupby(\"age_group\")[\"hosp_yn\"].apply(lambda h:(h==\"Yes\").sum()).reset_index(name='hosp_count')\n",
    "hosp_by_age_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X & Y for plotting bar chart\n",
    "hospitalizations = hosp_by_age_group[\"hosp_count\"].tolist()\n",
    "age_group = hosp_by_age_group[\"age_group\"].tolist()\n",
    "hospitalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart for plotting correlation between \"number of hospitalizations\" by \"Age_group\"\n",
    "fig, ax = plt.subplots(figsize=(8,8)) \n",
    "ax.bar(age_group, hospitalizations,color=\"skyblue\", label=\"Hospitalizations By Age_Group\")\n",
    "ax.tick_params(axis='x', rotation=80)\n",
    "ax.tick_params(axis='both', labelsize=8)\n",
    "plt.xlabel('Age_Group', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Hospitalizations', fontsize=12, fontweight='bold')\n",
    "plt.title(\"Hospitalizations_By_Age_Group\", fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "plt.savefig('Resources/Hospitalizations_By_Age_Group.png', transparent=False, facecolor=\"skyblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar charts showing correlation between sex & medcond and Race & medcond\n",
    "pd.crosstab(usa_df[\"sex\"], usa_df[\"medcond_yn\"]).plot(kind='bar')\n",
    "pd.crosstab(usa_df[\"race_ethnicity_combined\"], usa_df[\"medcond_yn\"]).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "usa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population \n",
    "# race_population_df = pd.read_csv(\"Resources/race_population.csv\")\n",
    "race_population_df = pd.read_csv(\"s3://population-csv-bucket/race_population.csv\", low_memory=False)\n",
    "race_population_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = race_population_df[\"population_distribution\"].mean()\n",
    "race_population_df.loc[len(race_population_df.index)] = ['Unknown', mean]  \n",
    "race_population_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postgres SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection to postgres DB that's hosted on AWS RDS\n",
    "db_string = f\"postgres://postgres:{db_password}@group-3.cey3rp5wgnme.us-east-2.rds.amazonaws.com:5432/postgres\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engine\n",
    "engine = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into sql DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into DB in chunks of rows\n",
    "rows_imported = 0\n",
    "for data in pd.read_csv(covid_19_data, chunksize=10000):\n",
    "\n",
    "        # print out the range of rows that are being imported\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "\n",
    "        data.to_sql(name='covid_19_data', con=engine, if_exists='append', index=False)\n",
    "\n",
    "        # increment the number of rows imported by the size of 'data'\n",
    "        rows_imported += len(data)\n",
    "\n",
    "        # print that the rows have finished importing\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load population dataframe into sql\n",
    "population = race_population_df.to_sql(name=\"population\", con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns list of dataframe\n",
    "usa_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the table from sql to a dataframe\n",
    "covid_19_data = pd.read_sql(\"covid_merged_population\", engine )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head on dataframe\n",
    "covid_19_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Labels to convert categorical data to numerical values to feed into ML model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "usa_df_binary_encoded = covid_19_data.copy()\n",
    "usa_df_binary_encoded['race_ethnicity_combined'] = le.fit_transform(usa_df_binary_encoded['race_ethnicity_combined'])\n",
    "usa_df_binary_encoded['sex'] = le.fit_transform(usa_df_binary_encoded['sex'])\n",
    "usa_df_binary_encoded['hosp_yn'] = le.fit_transform(usa_df_binary_encoded['hosp_yn'])\n",
    "usa_df_binary_encoded['icu_yn'] = le.fit_transform(usa_df_binary_encoded['icu_yn'])\n",
    "usa_df_binary_encoded['death_yn'] = le.fit_transform(usa_df_binary_encoded['death_yn'])\n",
    "usa_df_binary_encoded['medcond_yn'] = le.fit_transform(usa_df_binary_encoded['medcond_yn'])\n",
    "usa_df_binary_encoded['age_group'] = le.fit_transform(usa_df_binary_encoded['age_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoded dataframe\n",
    "usa_df_binary_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features(X) and y\n",
    "y = usa_df_binary_encoded[\"death_yn\"]\n",
    "X = usa_df_binary_encoded.drop([\"cdc_case_earliest_dt \",\"death_yn\",\"cdc_report_dt\",\"pos_spec_dt\", \"onset_dt\",\"current_status\", \"population_distribution\"], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Test sets\n",
    "#using 80/20 rule\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "   y, train_size = .80, test_size= .20, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Logistic Regressin model\n",
    "classifier = LogisticRegression(solver='lbfgs',\n",
    "   max_iter=200,\n",
    "   random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions using the testing data.\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing predictions vs actual\n",
    "print('True', y_test.values[0:25])\n",
    "print('Pred', y_pred[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "#difference between actual and estimated values.\n",
    "#small value indticates better model, larger values indicates larger error (i.e senstive to outliers)\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print coef & intercept\n",
    "print(classifier.coef_)\n",
    "print(classifier.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score\n",
    "log_acc = accuracy_score(y_test, y_pred)\n",
    "print(log_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "log_report = classification_report(y_test, y_pred)\n",
    "print(\"Logistics Regression\")\n",
    "print(log_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[\"age_group\"],y_pred)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The receiver operating characteristic (ROC) for logistic regression\n",
    "logit_roc_auc = roc_auc_score(y_test, classifier.predict(X_test))\n",
    "fpr_logit, tpr_logit, thresholds = roc_curve(y_test, classifier.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr_logit, tpr_logit, marker=\".\",label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a StandardScaler instance.\n",
    "scaler = StandardScaler()\n",
    "# Fitting the Standard Scaler with the training data.\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scaling the data.\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=128, random_state=78) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions using the testing data.\n",
    "rf_predictions = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix.\n",
    "cm = confusion_matrix(y_test, rf_predictions)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy score.\n",
    "rf_acc_score = accuracy_score(y_test, rf_predictions)\n",
    "rf_acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {rf_acc_score}\")\n",
    "print(\"Random Forest Classification Report\")\n",
    "print(classification_report(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest classification report \n",
    "rf_report = classification_report(y_test, rf_predictions)\n",
    "print(rf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve for Random Forest Model\n",
    "rf_model_probs = rf_model.predict_proba(X_test_scaled)\n",
    "rf_model_probs = rf_model_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_auc = roc_auc_score(y_test, rf_model_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, rf_model_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, marker='.', label='Random Forest Classification (area = %0.2f)' % rf_auc)\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The receiver operating characteristic (ROC) for both Logistic regression and Random Forest\n",
    "plt.figure()\n",
    "plt.plot(fpr_logit, tpr_logit, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot(fpr, tpr, label='Random Forest Classification (area = %0.2f)' % rf_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking the Importance of Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance in the Random Forest model.\n",
    "importances = rf_model.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sort the features by their importance.\n",
    "sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the decision tree classifier instance.\n",
    "\n",
    "tree_model = tree.DecisionTreeClassifier()\n",
    "# Fitting the model.\n",
    "tree_model = tree_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions using the testing data.\n",
    "tree_predictions = tree_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix\n",
    "tree_cm = confusion_matrix(y_test, tree_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the confusion matrix.\n",
    "tree_cm_df = pd.DataFrame(\n",
    "    tree_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "tree_cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy score.\n",
    "tree_acc_score = accuracy_score(y_test, tree_predictions)\n",
    "tree_acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(tree_cm_df)\n",
    "print(f\"Accuracy Score : {tree_acc_score}\")\n",
    "print(\"Decision Tree Classification Report\")\n",
    "print(classification_report(y_test, tree_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree report\n",
    "tree_report = classification_report(y_test, tree_predictions)\n",
    "print(tree_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve for Decision Tree Model\n",
    "tree_model_probs = tree_model.predict_proba(X_test_scaled)\n",
    "tree_model_probs = tree_model_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_auc = roc_auc_score(y_test, tree_model_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_tree, tp_tree, _ = roc_curve(y_test, tree_model_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fp_tree, tp_tree, marker='.', label='Decision Tree Classification (area = %0.2f)' % tree_auc)\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The receiver operating characteristic (ROC) for Logistic regression, Random Forest and Decision Tree\n",
    "plt.figure()\n",
    "plt.plot(fpr_logit, tpr_logit, label='Logistic Regression (area = %0.3f)' % logit_roc_auc)\n",
    "plt.plot(fpr, tpr, label='Random Forest Classification (area = %0.3f)' % rf_auc)\n",
    "plt.plot(fp_tree, tp_tree, label='Decision Tree Classification (area = %0.3f)' % tree_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Resources/ROC.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Accuracy Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'Model' :['Logistics Regression',\n",
    "               'Random Forest',\n",
    "               'Decision Tree'],\n",
    "    'Score': [log_acc,\n",
    "             rf_acc_score,\n",
    "             tree_acc_score]\n",
    "}\n",
    "models_df = pd.DataFrame(data=d)\n",
    "models_df.sort_values(by = \"Score\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData_3.7",
   "language": "python",
   "name": "pythondata_3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
